{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3659b4b4-e2ea-495a-a6f6-136c8922ed96",
   "metadata": {},
   "source": [
    "Reference code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4215ac9-e4b0-46db-a38a-a66139ed4a71",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2161878846.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[6], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    '''\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "#use this to setup and run MTEB\n",
    "'''\n",
    "model = SentenceTransformer(model_name)\n",
    "tasks = mteb.get_tasks(tasks=[\"AskUbuntuDupQuestions\", \"MindSmallReranking\", \"SciDocsRR\", \"StackOverflowDupQuestions\"])\n",
    "evaluation = mteb.MTEB(tasks=tasks)\n",
    "results = evaluation.run(model, output_folder=f\"results/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655b9e7a-9e9d-4a7b-98f5-319f469f16fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How FlagOpen ran it:\n",
    "#  Source: FlagEmbedding/C_MTEB/eval_cross_encoder.py\n",
    "'''\n",
    "if __name__ == '__main__':\n",
    "    args = get_args()\n",
    "\n",
    "    model = FlagReranker(args.model_name_or_path, use_fp16=True)\n",
    "\n",
    "    if 'checkpoint-' in args.model_name_or_path:\n",
    "        save_name = \"_\".join(args.model_name_or_path.split('/')[-2:])\n",
    "    else:\n",
    "        save_name = \"_\".join(args.model_name_or_path.split('/')[-1:])\n",
    "\n",
    "    evaluation = MTEB(task_types=[\"Reranking\"], task_langs=['zh', 'zh2en', 'en2zh'])\n",
    "    evaluation.run(model, output_folder=f\"reranker_results/{save_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5281063d-8464-48b8-8dde-060636ece877",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import FlagReranker\n",
    "reranker = FlagReranker('BAAI/bge-reranker-large', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n",
    "\n",
    "score = reranker.compute_score(['query', 'passage'])\n",
    "print(score)\n",
    "\n",
    "scores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']])\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3ad7db-ccf1-481a-96e2-2483284d3976",
   "metadata": {},
   "source": [
    "Code I will use to run:\n",
    "\n",
    "-------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1e359b5-5827-45a7-a1d4-165b510009a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\.conda\\envs\\HII\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import mteb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from FlagEmbedding import FlagReranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a968cb5-3f8e-46be-8b7f-ba792be252e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `task_langs` argument is deprecated and will be removed in the next release. Please use `tasks = mteb.get_tasks(... languages = [...])` to filter tasks instead. Note that this uses 3 letter language codes (ISO 639-3).\n",
      "Please supply model revision.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #262626; text-decoration-color: #262626\">───────────────────────────────────────────────── </span><span style=\"font-weight: bold\">Selected tasks </span><span style=\"color: #262626; text-decoration-color: #262626\"> ─────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;5;235m───────────────────────────────────────────────── \u001b[0m\u001b[1mSelected tasks \u001b[0m\u001b[38;5;235m ─────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Reranking</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mReranking\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    - AskUbuntuDupQuestions, <span style=\"color: #626262; text-decoration-color: #626262; font-style: italic\">s2s</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "    - AskUbuntuDupQuestions, \u001b[3;38;5;241ms2s\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    - MindSmallReranking, <span style=\"color: #626262; text-decoration-color: #626262; font-style: italic\">s2s</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "    - MindSmallReranking, \u001b[3;38;5;241ms2s\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    - SciDocsRR, <span style=\"color: #626262; text-decoration-color: #626262; font-style: italic\">s2s</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "    - SciDocsRR, \u001b[3;38;5;241ms2s\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    - StackOverflowDupQuestions, <span style=\"color: #626262; text-decoration-color: #626262; font-style: italic\">s2s</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "    - StackOverflowDupQuestions, \u001b[3;38;5;241ms2s\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while evaluating AskUbuntuDupQuestions: 'FlagReranker' object has no attribute 'encode'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FlagReranker' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#run and save file\u001b[39;00m\n\u001b[0;32m      7\u001b[0m save_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbge-reranker-large\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mevaluation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreranker_results/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msave_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\HII\\Lib\\site-packages\\mteb\\evaluation\\MTEB.py:411\u001b[0m, in \u001b[0;36mMTEB.run\u001b[1;34m(self, model, verbosity, output_folder, eval_splits, overwrite_results, raise_error, co2_tracker, **kwargs)\u001b[0m\n\u001b[0;32m    407\u001b[0m logger\u001b[38;5;241m.\u001b[39merror(\n\u001b[0;32m    408\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while evaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;241m.\u001b[39mmetadata_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    409\u001b[0m )\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_error:\n\u001b[1;32m--> 411\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    412\u001b[0m logger\u001b[38;5;241m.\u001b[39merror(\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check all the error logs at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merr_logs_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    414\u001b[0m )\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merr_logs_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f_out:\n",
      "File \u001b[1;32m~\\.conda\\envs\\HII\\Lib\\site-packages\\mteb\\evaluation\\MTEB.py:377\u001b[0m, in \u001b[0;36mMTEB.run\u001b[1;34m(self, model, verbosity, output_folder, eval_splits, overwrite_results, raise_error, co2_tracker, **kwargs)\u001b[0m\n\u001b[0;32m    373\u001b[0m     kg_co2_emissions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    374\u001b[0m         tracker\u001b[38;5;241m.\u001b[39mfinal_emissions\n\u001b[0;32m    375\u001b[0m     )  \u001b[38;5;66;03m# expressed as kilograms of CO₂-equivalents\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 377\u001b[0m     results, tick, tock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    381\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;241m.\u001b[39mmetadata_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtock\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mtick\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m )\n\u001b[0;32m    384\u001b[0m evaluation_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tock \u001b[38;5;241m-\u001b[39m tick\n",
      "File \u001b[1;32m~\\.conda\\envs\\HII\\Lib\\site-packages\\mteb\\evaluation\\MTEB.py:260\u001b[0m, in \u001b[0;36mMTEB._run_eval\u001b[1;34m(task, model, split, output_folder, **kwargs)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_eval\u001b[39m(task, model, split, output_folder, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    259\u001b[0m     tick \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m--> 260\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    261\u001b[0m     tock \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results, tick, tock\n",
      "File \u001b[1;32m~\\.conda\\envs\\HII\\Lib\\site-packages\\mteb\\abstasks\\AbsTask.py:115\u001b[0m, in \u001b[0;36mAbsTask.evaluate\u001b[1;34m(self, model, split, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    114\u001b[0m         data_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[hf_subset][split]\n\u001b[1;32m--> 115\u001b[0m     scores[hf_subset] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate_subset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[1;32m~\\.conda\\envs\\HII\\Lib\\site-packages\\mteb\\abstasks\\AbsTaskReranking.py:33\u001b[0m, in \u001b[0;36mAbsTaskReranking._evaluate_subset\u001b[1;34m(self, model, data_split, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_evaluate_subset\u001b[39m(\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     28\u001b[0m     model: Encoder \u001b[38;5;241m|\u001b[39m EncoderWithQueryCorpusEncode,\n\u001b[0;32m     29\u001b[0m     data_split: Dataset,\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m     31\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ScoresDict:\n\u001b[0;32m     32\u001b[0m     evaluator \u001b[38;5;241m=\u001b[39m RerankingEvaluator(data_split, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 33\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_main_score(scores)\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[1;32m~\\.conda\\envs\\HII\\Lib\\site-packages\\mteb\\evaluation\\evaluators\\RerankingEvaluator.py:61\u001b[0m, in \u001b[0;36mRerankingEvaluator.__call__\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model):\n\u001b[1;32m---> 61\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[1;32m~\\.conda\\envs\\HII\\Lib\\site-packages\\mteb\\evaluation\\evaluators\\RerankingEvaluator.py:66\u001b[0m, in \u001b[0;36mRerankingEvaluator.compute_metrics\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_metrics\u001b[39m(\u001b[38;5;28mself\u001b[39m, model):\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m---> 66\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics_batched\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_batched_encoding\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics_individual(model)\n\u001b[0;32m     69\u001b[0m     )\n",
      "File \u001b[1;32m~\\.conda\\envs\\HII\\Lib\\site-packages\\mteb\\evaluation\\evaluators\\RerankingEvaluator.py:84\u001b[0m, in \u001b[0;36mRerankingEvaluator.compute_metrics_batched\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m     77\u001b[0m all_conf_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# using encode_queries and encode_corpus functions if they exists,\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# which can be defined by users to add different instructions for query and passage conveniently\u001b[39;00m\n\u001b[0;32m     81\u001b[0m encode_queries_func \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     82\u001b[0m     model\u001b[38;5;241m.\u001b[39mencode_queries\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, EncoderWithQueryCorpusEncode)\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\n\u001b[0;32m     85\u001b[0m )\n\u001b[0;32m     86\u001b[0m encode_corpus_func \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     87\u001b[0m     model\u001b[38;5;241m.\u001b[39mencode_corpus\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, EncoderWithQueryCorpusEncode)\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m model\u001b[38;5;241m.\u001b[39mencode\n\u001b[0;32m     90\u001b[0m )\n\u001b[0;32m     92\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoding queries...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'FlagReranker' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "#set parameters/model\n",
    "model = FlagReranker('BAAI/bge-reranker-large', use_fp16=True)\n",
    "tasks = mteb.get_tasks(tasks=[\"AskUbuntuDupQuestions\", \"MindSmallReranking\", \"SciDocsRR\", \"StackOverflowDupQuestions\"])\n",
    "evaluation = mteb.MTEB(tasks=tasks, task_langs=['eng'])\n",
    "\n",
    "#run and save file\n",
    "save_name = \"bge-reranker-large\"\n",
    "evaluation.run(model, output_folder=f\"reranker_results/{save_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86328162-387a-4bd0-98f9-f4af0dd952bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
